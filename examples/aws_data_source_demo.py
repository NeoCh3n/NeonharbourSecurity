#!/usr/bin/env python3
"""
Demo script showing how to use AWS data source connectors.

This script demonstrates how to fetch data from CloudTrail, VPC Flow Logs,
and GuardDuty using the new AWS connectors for the Interactive Demo System.
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Any

# Add the project root to the Python path
sys.path.insert(0, str(Path(__file__).parent.parent))

from src.connectors.cloudtrail import CloudTrailClient
from src.connectors.vpcflow import VPCFlowLogsClient
from src.connectors.guardduty import GuardDutyClient


def fetch_security_data() -> Dict[str, List[Dict[str, Any]]]:
    """Fetch security data from all AWS sources."""
    
    # Initialize connectors (will use fixture data if AWS credentials not available)
    cloudtrail_client = CloudTrailClient(fixture_dir=Path("tools/seed"))
    vpcflow_client = VPCFlowLogsClient(fixture_dir=Path("tools/seed"))
    guardduty_client = GuardDutyClient(fixture_dir=Path("tools/seed"))
    
    print("üîç Fetching security data from AWS sources...")
    
    # Fetch CloudTrail events
    print("\nüìã CloudTrail Events:")
    cloudtrail_events = cloudtrail_client.fetch_security_events(limit=5)
    print(f"  Found {len(cloudtrail_events)} security events")
    
    failed_logins = cloudtrail_client.fetch_failed_logins(limit=3)
    print(f"  Found {len(failed_logins)} failed login attempts")
    
    # Fetch VPC Flow Logs
    print("\nüåê VPC Flow Logs:")
    flow_logs = vpcflow_client.fetch_recent_flow_logs(limit=5)
    print(f"  Found {len(flow_logs)} flow log entries")
    
    suspicious_traffic = vpcflow_client.fetch_suspicious_traffic(limit=3)
    print(f"  Found {len(suspicious_traffic)} suspicious traffic patterns")
    
    rejected_connections = vpcflow_client.fetch_rejected_connections(limit=3)
    print(f"  Found {len(rejected_connections)} rejected connections")
    
    # Fetch GuardDuty findings
    print("\nüõ°Ô∏è  GuardDuty Findings:")
    guardduty_findings = guardduty_client.fetch_recent_findings(limit=5)
    print(f"  Found {len(guardduty_findings)} total findings")
    
    high_severity_findings = guardduty_client.fetch_high_severity_findings(limit=3)
    print(f"  Found {len(high_severity_findings)} high severity findings")
    
    malware_findings = guardduty_client.fetch_malware_findings(limit=2)
    print(f"  Found {len(malware_findings)} malware findings")
    
    crypto_findings = guardduty_client.fetch_cryptocurrency_findings(limit=2)
    print(f"  Found {len(crypto_findings)} cryptocurrency mining findings")
    
    # Clean up
    cloudtrail_client.close()
    vpcflow_client.close()
    guardduty_client.close()
    
    return {
        "cloudtrail_events": cloudtrail_events,
        "failed_logins": failed_logins,
        "flow_logs": flow_logs,
        "suspicious_traffic": suspicious_traffic,
        "rejected_connections": rejected_connections,
        "guardduty_findings": guardduty_findings,
        "high_severity_findings": high_severity_findings,
        "malware_findings": malware_findings,
        "crypto_findings": crypto_findings
    }


def analyze_risk_scores(data: Dict[str, List[Dict[str, Any]]]) -> None:
    """Analyze risk scores across all data sources."""
    from collections.abc import Iterable
    
    def normalize_iterable(records):
        """ÊääÂêÑÁßçÂèØËÉΩÁöÑËÆ∞ÂΩïÂÆπÂô®Áªü‰∏ÄÊàêÂèØËø≠‰ª£ÁöÑ list„ÄÇ"""
        if records is None:
            return []
        if isinstance(records, dict):
            if "items" in records and isinstance(records["items"], Iterable):
                return list(records["items"])
            return list(records.values())
        if isinstance(records, (list, tuple, set)):
            return list(records)
        if isinstance(records, (str, bytes)):
            return []
        return []
    
    print("\nüìä Risk Score Analysis:")
    
    all_items = []
    for source_name, records in data.items():
        iterable_records = normalize_iterable(records)
        for item in iterable_records:
            if isinstance(item, dict):
                all_items.append({
                    "source": source_name,
                    "risk_score": item.get("risk_score", 0.0),
                    "severity": item.get("severity", "unknown"),
                    "type": item.get("event_name") or item.get("type") or item.get("protocol_name", "unknown")
                })
    
    # Sort by risk score (highest first)
    all_items.sort(key=lambda x: x["risk_score"], reverse=True)
    
    print(f"\nüî• Top 10 Highest Risk Items:")
    for i, item in enumerate(all_items[:10], 1):
        print(f"  {i:2d}. {item['source']:20} | Risk: {item['risk_score']:.2f} | "
              f"Severity: {item['severity']:6} | Type: {item['type']}")
    
    # Risk distribution
    high_risk = len([item for item in all_items if item["risk_score"] >= 0.7])
    medium_risk = len([item for item in all_items if 0.3 <= item["risk_score"] < 0.7])
    low_risk = len([item for item in all_items if item["risk_score"] < 0.3])
    
    print(f"\nüìà Risk Distribution:")
    print(f"  High Risk (‚â•0.7):   {high_risk:3d} items ({high_risk/len(all_items)*100:.1f}%)")
    print(f"  Medium Risk (0.3-0.7): {medium_risk:3d} items ({medium_risk/len(all_items)*100:.1f}%)")
    print(f"  Low Risk (<0.3):     {low_risk:3d} items ({low_risk/len(all_items)*100:.1f}%)")


def demonstrate_investigation_pipeline_integration(data: Dict[str, List[Dict[str, Any]]]) -> None:
    """Demonstrate how the data would be processed by the investigation pipeline."""
    
    print("\nüîÑ Investigation Pipeline Integration:")
    
    # Simulate how high-risk items would be processed
    from collections.abc import Iterable
    
    def normalize_iterable(records):
        """ÊääÂêÑÁßçÂèØËÉΩÁöÑËÆ∞ÂΩïÂÆπÂô®Áªü‰∏ÄÊàêÂèØËø≠‰ª£ÁöÑ list„ÄÇ"""
        if records is None:
            return []
        if isinstance(records, dict):
            if "items" in records and isinstance(records["items"], Iterable):
                return list(records["items"])
            return list(records.values())
        if isinstance(records, (list, tuple, set)):
            return list(records)
        if isinstance(records, (str, bytes)):
            return []
        return []
    
    # Collect ALL unique items first to avoid double counting
    all_unique_items = {}  # Use dict to deduplicate by ID
    
    for source_name, records in data.items():
        iterable_records = normalize_iterable(records)
        for item in iterable_records:
            if isinstance(item, dict):
                # Create unique key for deduplication
                item_id = item.get("id") or f"{source_name}_{hash(str(item))}"
                if item_id not in all_unique_items:
                    all_unique_items[item_id] = {
                        "source": source_name,
                        "data": item
                    }
    
    # Now analyze the unique items
    high_risk_items = []
    auto_closable = 0
    total_items = len(all_unique_items)
    
    for item_id, item_info in all_unique_items.items():
        source_name = item_info["source"]
        item = item_info["data"]
        risk_score = item.get("risk_score", 0.0)
        
        # Check if high risk
        if risk_score >= 0.7:
            high_risk_items.append({
                "source": source_name,
                "data": item,
                "investigation_priority": "high" if risk_score >= 0.8 else "medium"
            })
        
        # Check if auto-closable (medium and low risk can be auto-processed)
        if risk_score < 0.7:  # Changed from 0.3 to 0.7 to include medium risk
            auto_closable += 1
    
    print(f"\nüìä Item Count Analysis:")
    print(f"  Raw items from all sources: {sum(len(normalize_iterable(records)) for records in data.values())}")
    print(f"  Unique items (after deduplication): {total_items}")
    print(f"  High-risk items (‚â•0.7): {len(high_risk_items)}")
    print(f"  Auto-closable items (<0.3): {auto_closable}")
    
    print(f"\n‚ö° {len(high_risk_items)} high-risk items would trigger investigations:")
    
    for item in high_risk_items:
        source = item["source"]
        data = item["data"]
        priority = item["investigation_priority"]
        
        # Extract key identifying information
        if source.startswith("cloudtrail"):
            identifier = f"{data.get('event_name')} from {data.get('source_ip')}"
        elif source.startswith("flow_logs") or source == "suspicious_traffic" or source == "rejected_connections":
            identifier = f"{data.get('protocol_name')} {data.get('srcaddr')}‚Üí{data.get('dstaddr')}:{data.get('dstport')}"
        else:  # GuardDuty
            identifier = f"{data.get('type')} (Confidence: {data.get('confidence', 0):.1f})"
        
        print(f"  ‚Ä¢ {priority.upper():6} | {source:20} | {identifier}")
    
    # Automation simulation with corrected counts
    automation_rate = (auto_closable / total_items * 100) if total_items > 0 else 0
    
    print(f"\nü§ñ Automation Simulation:")
    print(f"  Total unique items: {total_items}")
    print(f"  Auto-processable (risk < 0.7): {auto_closable}")
    print(f"  Require human investigation (risk ‚â• 0.7): {total_items - auto_closable}")
    print(f"  Automation rate: {automation_rate:.1f}%")
    
    if automation_rate >= 80:
        print("  ‚úÖ Target 80%+ automation rate achieved!")
    else:
        print("  ‚ö†Ô∏è  Below 80% automation target")


def main():
    """Main demo function."""
    
    print("üöÄ AWS Data Source Connectors Demo")
    print("=" * 50)
    
    try:
        # Fetch all security data
        security_data = fetch_security_data()
        
        # Analyze risk scores
        analyze_risk_scores(security_data)
        
        # Demonstrate investigation pipeline integration
        demonstrate_investigation_pipeline_integration(security_data)
        
        print("\n‚úÖ Demo completed successfully!")
        print("\nüí° Next steps:")
        print("  1. Configure AWS credentials to fetch live data")
        print("  2. Integrate with EventBridge for real-time ingestion")
        print("  3. Route high-risk items through Step Functions workflow")
        print("  4. Use in Interactive Demo System for live demonstrations")
        
    except Exception as e:
        print(f"\n‚ùå Demo failed: {e}")
        raise


if __name__ == "__main__":
    main()